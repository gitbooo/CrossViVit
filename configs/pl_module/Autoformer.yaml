_target_: src.pl_modules.pl_baselines.TSForecastTask

name: Autoformer
optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 0.0016
  weight_decay: 0.05
  betas: [0.9, 0.95]

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.5
  patience: 10

model:
  _target_: src.models.Autoformer.Autoformer
  enc_in: 10 # Input size of encoder
  dec_in: 10 # Input size of decoder
  c_out: 1 # Output size
  seq_len: 48 # Length of input sequence
  label_len: 24 # Length of label sequence
  pred_len: 48 # Length of prediction sequence
  moving_avg: 25 # Moving average window size
  d_model: 512 # Dimension of the model
  n_heads: 8 # Number of heads
  e_layers: 3 # Number of encoder layers
  d_layers: 2 # Number of decoder layers
  d_ff: 2048 # Dimension of FCN
  factor: 5 # ProbSparse Attention factor
  dropout: 0.05 # Dropout probability
  embed: timeF # Type of time features encoding [timeF, fixed, learned]
  activation: gelu # Activation function 
  output_attention: False # Whether to output attention in the encoder

metrics:
  train:
    rmse:
      _target_: torchmetrics.MeanSquaredError
      squared: False
    mae:
      _target_: torchmetrics.MeanAbsoluteError
    mape:
      _target_: torchmetrics.MeanAbsolutePercentageError
      epsilon: 1
  val:
    rmse:
      _target_: torchmetrics.MeanSquaredError
      squared: False
    mae:
      _target_: torchmetrics.MeanAbsoluteError
    mape:
      _target_: torchmetrics.MeanAbsolutePercentageError
      epsilon: 1
criterion: 
  _target_: torch.nn.MSELoss
monitor: val/rmse

seq_len: 48
label_len: 24
pred_len: 48
padding: 0
inverse_scaling: False
output_attention: False
scaler: None
  