_target_: src.pl_modules.pl_baselines.TSForecastTask

name: Autoformer
optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 0.0016
  weight_decay: 0.05
  betas: [0.9, 0.95]

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.5
  patience: 10

model:
  _target_: src.models.Crossformer.Model
  enc_in: 10 # Input size of encoder
  seq_len: 48 # Length of input sequence
  pred_len: 48 # Length of prediction sequence
  d_model: 512 # Dimension of the model
  n_heads: 8 # Number of heads
  e_layers: 3 # Number of encoder layers
  d_ff: 2048 # Dimension of FCN
  factor: 5 # ProbSparse Attention factor
  dropout: 0.05 # Dropout probability
  output_attention: False # Whether to output attention weights

metrics:
  train:
    rmse:
      _target_: torchmetrics.MeanSquaredError
      squared: False
    mae:
      _target_: torchmetrics.MeanAbsoluteError
  val:
    rmse:
      _target_: torchmetrics.MeanSquaredError
      squared: False
    mae:
      _target_: torchmetrics.MeanAbsoluteError
criterion: 
  _target_: torch.nn.MSELoss
monitor: val/rmse

seq_len: 48
label_len: 24
pred_len: 48
padding: 0
inverse_scaling: False
output_attention: False
scaler: None
  